# job_count_scraping
求人サイトからカテゴリ別に求人数を取得し、csvに出力する

# 注意
本リポジトリで提供されているスクレイピングスクリプトは、各求人サイトのカテゴリ別求人数を取得し、私的利用の範囲で分析・整理することを目的としています。以下の点に十分ご注意ください。

各サイトの利用規約を必ずご確認ください。
サイトによってはスクレイピングを禁止している場合があります。利用規約に反する形での使用は行わないでください。

過度なアクセスを避けてください。
サーバーに負荷をかけないよう、適切なリクエスト間隔（スリープ処理など）を設けるようにしてください。

取得したデータは私的利用の範囲でご利用ください。
商用利用や第三者への再配布、公開などは各サイトの規約に従ってください。

サイト構造の変更により、スクリプトが正常に動作しなくなる可能性があります。
その場合は、必要に応じてスクリプトの修正を行ってください。


# 手順

1. pythonをインストール

    公式サイトから最新版をダウンロード
    https://www.python.org/downloads/

    インストール後にターミナルを開いて
    
    ```
    python -v
    ```

    でインストールされているかを確認

    参考サイト：https://qiita.com/yoshi-kin/items/e0a7336a288188913097

2. seleniumをインストール

    ターミナルを開いて

    ```
    pip install selenium
    ```

3. chromeのドライバーをインストール

    chromeのバージョンを確認。

    確認方法
    https://mhtdesign.net/guide/version-confirmation.html

    ```
    # 94のところをchromeのバージョンにする
    pip install chromedriver-binary==94.*
    ```
4. run_all.pyを実行する。

    ```
    pythonのpath run_all.pyのpath

    例
    /Users/yu-suzuki/Project/job_count_csv/venv/bin/python /Users/yu-suzuki/Project/j
ob_count_csv/run_all.py
    ```

5. data配下にcsvが出力されるので、それをスプレッドシートにインポート


## 備考

### 注意点
run_all.pyが実行時間30分~1時間ほどかかる。
ディスプレイがスリープになったら中断されるので、要設定。かつCPUも食うかもしれないので、他作業がない時にでも実行推奨。

失敗の際は正常終了が出力されないので、そこでエラーを確認。

chromeをアップデートするとdriverと差異が出て動かなくなる可能性あり、その場合は更新したバージョンのdriverを再度インストールする必要。
